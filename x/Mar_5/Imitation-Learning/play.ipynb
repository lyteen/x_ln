{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test BC (Behavior Cloning)\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim: int,\n",
    "            action_dim: int,\n",
    "            hidden_dim: int=128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "# collect expert data\n",
    "\"\"\"\n",
    "    An episode refers to one full sequence of interactions \n",
    "    between an agent and an environment in reinforcement learning\n",
    "\"\"\"\n",
    "def collect_expert_data(\n",
    "        env: object,\n",
    "        expert_policy: object,\n",
    "        episodes: int=5\n",
    ") -> object:\n",
    "    data = []\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = expert_policy(state)\n",
    "            data.append((state, action))\n",
    "            \"\"\"\n",
    "                openAI Gym env.step(action) Param:\n",
    "                1.state: new state of the environment after taking action\n",
    "                2.reward: action reward\n",
    "                3.done: episodes is Done (bool)\n",
    "                4.info: a dictionary containing additional information about the environment \n",
    "            \"\"\"\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    return data \n",
    "\n",
    "# training\n",
    "def train_bc(\n",
    "        policy_net: nn.Module, \n",
    "        data: object, \n",
    "        epochs: int=5,\n",
    "        lr: float=1e-3\n",
    "    ):\n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    states, actions = zip(*data)\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states)\n",
    "        loss = criterion(outputs, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# evaluate BC_model performance\n",
    "def evaluate_policy(env, policy_net, episodes=10):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                action = torch.argmax(policy_net(state_tensor)).item()\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "        success += 1\n",
    "    print(f\"Success Rate: {success}/{episodes}\")\n",
    "\n",
    "# define eport policy\n",
    "def expert_policy(state):\n",
    "    return 1 if state[2] > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6939858794212341\n",
      "Epoch 10, Loss: 0.6743330359458923\n",
      "Epoch 20, Loss: 0.6561333537101746\n",
      "Epoch 30, Loss: 0.6373411417007446\n",
      "Epoch 40, Loss: 0.617154598236084\n",
      "Epoch 50, Loss: 0.5952470898628235\n",
      "Epoch 60, Loss: 0.5713289976119995\n",
      "Epoch 70, Loss: 0.5453448295593262\n",
      "Epoch 80, Loss: 0.517549991607666\n",
      "Epoch 90, Loss: 0.4889135956764221\n",
      "Epoch 100, Loss: 0.4601045846939087\n",
      "Epoch 110, Loss: 0.4315755367279053\n",
      "Epoch 120, Loss: 0.40371987223625183\n",
      "Epoch 130, Loss: 0.37671494483947754\n",
      "Epoch 140, Loss: 0.35152238607406616\n",
      "Epoch 150, Loss: 0.3289012312889099\n",
      "Epoch 160, Loss: 0.30868056416511536\n",
      "Epoch 170, Loss: 0.29051753878593445\n",
      "Epoch 180, Loss: 0.27415862679481506\n",
      "Epoch 190, Loss: 0.2598109245300293\n",
      "Success Rate: 10/10\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "expert_data = collect_expert_data(env, expert_policy, episodes=20)\n",
    "\n",
    "policy_net = PolicyNet(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim\n",
    ")\n",
    "train_bc(policy_net, expert_data, epochs=200)\n",
    "evaluate_policy(env, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the reinforcement environment\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set environmrnt\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state = env.reset()\n",
    "\n",
    "# get the rendered image\n",
    "frame = env.render()\n",
    "\n",
    "# show using matplotlib\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use DAgger expert can continue guide agency\n",
    "# Not Done\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 策略网络\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 采集专家数据\n",
    "def collect_expert_data(env, expert_policy, episodes=10):\n",
    "    data = []\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = expert_policy(state)\n",
    "            data.append((state, action))\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    return data\n",
    "\n",
    "# 训练 BC（Behavior Cloning）\n",
    "def train_bc(policy_net, data, epochs=100, lr=1e-3):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    states, actions = zip(*data)\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states)\n",
    "        loss = criterion(outputs, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 评估策略\n",
    "def evaluate_policy(env, policy_net, episodes=10):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                action = torch.argmax(policy_net(state_tensor)).item()\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "        success += 1\n",
    "    print(f\"Success Rate: {success}/{episodes}\")\n",
    "\n",
    "# 专家策略\n",
    "def expert_policy(state):\n",
    "    return 1 if state[2] > 0 else 0  # 根据杆子的角度选择方向\n",
    "\n",
    "# **DAgger**\n",
    "def dagger(env, expert_policy, policy_net, iterations=5, bc_epochs=100):\n",
    "    expert_data = collect_expert_data(env, expert_policy, episodes=20)\n",
    "    \n",
    "    # 先训练初始策略\n",
    "    train_bc(policy_net, expert_data, epochs=bc_epochs)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        new_data = []\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                action = torch.argmax(policy_net(state_tensor)).item()\n",
    "            \n",
    "            expert_action = expert_policy(state)  # 让专家修正\n",
    "            new_data.append((state, expert_action))\n",
    "\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "        \n",
    "        # 合并新数据并训练\n",
    "        expert_data.extend(new_data)\n",
    "        train_bc(policy_net, expert_data, epochs=bc_epochs)\n",
    "\n",
    "        print(f\"DAgger Iteration {i + 1} Completed\")\n",
    "        evaluate_policy(env, policy_net)\n",
    "\n",
    "# 运行 DAgger\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNet(state_dim, action_dim)\n",
    "dagger(env, expert_policy, policy_net, iterations=5, bc_epochs=100)\n",
    "\n",
    "# 最终评估\n",
    "evaluate_policy(env, policy_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m expert_trajectories \u001b[38;5;241m=\u001b[39m collect_expert_data(env, expert_policy, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# 运行最大熵 IRL 获取奖励\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m irl_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mmaxent_irl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpert_trajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# 训练 RL Agent\u001b[39;00m\n\u001b[0;32m    154\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m PolicyNet(num_states, num_actions)\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mmaxent_irl\u001b[1;34m(trajectories, num_states, num_actions, gamma, lr, epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_states)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# 迭代求解 V(s)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     V \u001b[38;5;241m=\u001b[39m logsumexp(rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m[:, :, \u001b[38;5;241m0\u001b[39m], V), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 计算状态-动作值函数 Q(s, a)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m Q \u001b[38;5;241m=\u001b[39m rewards[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(env\u001b[38;5;241m.\u001b[39mP[:, :, \u001b[38;5;241m0\u001b[39m], V)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TimeLimit' object has no attribute 'P'"
     ]
    }
   ],
   "source": [
    "# test IRL(Inverse Reinforcement Learning) IRL can inverse deriving the reward function, \n",
    "# which can make agency learn expert unknowed\n",
    "# TODO\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "# **1. 收集专家数据**\n",
    "def collect_expert_data(env, expert_policy, episodes=10):\n",
    "    trajectories = []\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        trajectory = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = expert_policy[state]\n",
    "            trajectory.append((state, action))\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "# **2. 最大熵 IRL 计算奖励**\n",
    "def maxent_irl(trajectories, num_states, num_actions, gamma=0.99, lr=0.1, epochs=100):\n",
    "    # 计算专家状态访问频率\n",
    "    state_visitation = np.zeros(num_states)\n",
    "    for traj in trajectories:\n",
    "        for (s, _) in traj:\n",
    "            state_visitation[s] += 1\n",
    "    state_visitation /= np.sum(state_visitation)\n",
    "\n",
    "    # 初始化奖励\n",
    "    rewards = np.random.rand(num_states)\n",
    "\n",
    "    # 迭代更新奖励\n",
    "    for epoch in range(epochs):\n",
    "        # 计算状态值函数 V(s)\n",
    "        V = np.zeros(num_states)\n",
    "        for _ in range(100):  # 迭代求解 V(s)\n",
    "            V = logsumexp(rewards + gamma * np.dot(env.P[:, :, 0], V), axis=1)\n",
    "        \n",
    "        # 计算状态-动作值函数 Q(s, a)\n",
    "        Q = rewards[:, None] + gamma * np.dot(env.P[:, :, 0], V)\n",
    "\n",
    "        # 计算策略 π(s) = softmax(Q)\n",
    "        policy = np.exp(Q - logsumexp(Q, axis=1, keepdims=True))\n",
    "\n",
    "        # 计算模型产生的状态访问频率\n",
    "        model_visitation = np.zeros(num_states)\n",
    "        state = env.reset()[0]\n",
    "        for _ in range(1000):\n",
    "            action = np.random.choice(num_actions, p=policy[state])\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            model_visitation[state] += 1\n",
    "            if done:\n",
    "                state = env.reset()[0]\n",
    "        model_visitation /= np.sum(model_visitation)\n",
    "\n",
    "        # 更新奖励\n",
    "        rewards += lr * (state_visitation - model_visitation)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Reward Update Done\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# **3. 使用 IRL 奖励训练 RL Agent**\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_rl(policy_net, rewards, env, lr=1e-2, epochs=500):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        states, actions, returns = [], [], []\n",
    "        G = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_probs = policy_net(state_tensor)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            G = rewards[state] + 0.99 * G\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            returns.append(G)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states)\n",
    "        loss = criterion(outputs, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: RL Training Loss {loss.item()}\")\n",
    "\n",
    "# **4. 评估训练的 RL 策略**\n",
    "def evaluate_policy(env, policy_net, episodes=10):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                action = torch.argmax(policy_net(state_tensor)).item()\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            success += reward\n",
    "    print(f\"Success Rate: {success}/{episodes}\")\n",
    "\n",
    "# \n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "# 获取状态和动作空间\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# **运行 IRL**\n",
    "# 定义专家策略（假设是最短路径策略）\n",
    "expert_policy = {\n",
    "    0: 1,  1: 2,  2: 1,  3: 2,\n",
    "    4: 1,  5: 2,  6: 1,  7: 2,\n",
    "    8: 1,  9: 2, 10: 1, 11: 2,\n",
    "    12: 1, 13: 2, 14: 1, 15: 2\n",
    "}\n",
    "\n",
    "# 采集专家轨迹\n",
    "expert_trajectories = collect_expert_data(env, expert_policy, episodes=20)\n",
    "\n",
    "# 运行最大熵 IRL 获取奖励\n",
    "irl_rewards = maxent_irl(expert_trajectories, num_states, num_actions)\n",
    "\n",
    "# 训练 RL Agent\n",
    "policy_net = PolicyNet(num_states, num_actions)\n",
    "train_rl(policy_net, irl_rewards, env)\n",
    "\n",
    "# 评估训练结果\n",
    "evaluate_policy(env, policy_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gan （GAIL）\n",
    "# Directly learn the expert’s policy without explicitly learning a reward function, better than DAgger、\n",
    "# TODO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# **1. 创建环境**\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# **2. 训练专家策略**\n",
    "expert_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "expert_model.learn(total_timesteps=50000)\n",
    "\n",
    "# **3. 采集专家轨迹**\n",
    "def collect_expert_data(env, model, episodes=10):\n",
    "    data = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            data.append((obs, action))\n",
    "            obs, _, done, _ = env.step(action)\n",
    "    return data\n",
    "\n",
    "expert_data = collect_expert_data(env, expert_model, episodes=20)\n",
    "\n",
    "# **4. 定义 GAIL 判别器**\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "# **5. 训练 GAIL**\n",
    "def train_gail(env, expert_data, iterations=5000, lr=1e-3):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = 1  # CartPole 的动作是离散的（0 or 1）\n",
    "\n",
    "    discriminator = Discriminator(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    agent = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # **生成 agent 轨迹**\n",
    "        agent_data = collect_expert_data(env, agent, episodes=5)\n",
    "\n",
    "        # **准备训练数据**\n",
    "        states_expert = torch.tensor(np.array([s for s, _ in expert_data]), dtype=torch.float32)\n",
    "        actions_expert = torch.tensor(np.array([a for _, a in expert_data]), dtype=torch.float32).unsqueeze(-1)\n",
    "        labels_expert = torch.ones(len(expert_data), 1)  # Label=1 for expert\n",
    "\n",
    "        states_agent = torch.tensor(np.array([s for s, _ in agent_data]), dtype=torch.float32)\n",
    "        actions_agent = torch.tensor(np.array([a for _, a in agent_data]), dtype=torch.float32).unsqueeze(-1)\n",
    "        labels_agent = torch.zeros(len(agent_data), 1)  # Label=0 for agent\n",
    "\n",
    "        # **计算判别器损失**\n",
    "        expert_preds = discriminator(states_expert, actions_expert)\n",
    "        agent_preds = discriminator(states_agent, actions_agent)\n",
    "\n",
    "        loss_expert = criterion(expert_preds, labels_expert)\n",
    "        loss_agent = criterion(agent_preds, labels_agent)\n",
    "        loss = (loss_expert + loss_agent) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # **训练策略 π 使其最大化 D 误判的概率**\n",
    "        agent.learn(total_timesteps=1000)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Discriminator Loss: {loss.item()}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# **6. 运行 GAIL 训练**\n",
    "trained_agent = train_gail(env, expert_data)\n",
    "\n",
    "# **7. 评估 GAIL 训练后的策略**\n",
    "def evaluate_agent(env, model, episodes=10):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            success += reward\n",
    "    print(f\"Average Reward: {success / episodes}\")\n",
    "\n",
    "evaluate_agent(env, trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
