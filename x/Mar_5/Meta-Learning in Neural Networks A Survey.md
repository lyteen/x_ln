## Abstract
**Keywords:** Meta-learning, learning-to-learn, few-shot learning, transfer learning, neural architecture search

<details>
    <summary>关键词</summary>
    元学习、学习学习、小样本学习、迁移学习、神经架构搜索
</details>

**Abstract:** Meta-learning, or learning-to-learn, has attracted significant interest recently, differing from traditional AI that solves tasks from scratch with a fixed learning algorithm. Meta-learning seeks to improve the learning algorithm itself based on multiple learning experiences. This survey explores contemporary meta-learning, including its definitions, relationship to fields like transfer learning and hyperparameter optimization, and a new taxonomy for categorizing meta-learning methods. It surveys applications like few-shot learning and reinforcement learning, discussing current challenges and future research areas.

<details>
    <summary>摘要</summary>
    元学习或学习-去-学习最近引起了极大的关注，它不同于使用固定学习算法从零开始解决任务的传统AI。 元学习旨在基于多个学习经验来改进学习算法本身。 本综述探讨了当代的元学习，包括其定义、与迁移学习和超参数优化等领域的关系，以及一种用于分类元学习方法的新分类法。 它调查了小样本学习和强化学习等应用，讨论了当前挑战和未来研究领域。
</details>

**Main Methods:**
This survey provides a comprehensive overview of meta-learning methods, focusing on contemporary neural network meta-learning. The methods and concepts discussed include:

1.  **High-Level Problem Formalization:** Defining meta-learning in a way that allows comparison and context within the field.
2.  **Taxonomy:** The survey presents a new taxonomy using meta-representation, meta-objective, and meta-optimizer.
3.  **Meta-Representation:** Investigates the choice of meta-knowledge to meta-learn, including parameter initialization, optimizers, feed-forward models, metric-learning, loss/reward functions, architectures, and modules.
4.  **Meta-Optimizer:** Discusses outer-level optimization techniques like gradient descent, reinforcement learning, and evolutionary algorithms.
5.  **Meta-Objective and Episode Design:** Covers meta-learning goals through meta-objective choices, such as many-shot versus few-shot episodes, and multi-task versus single-task objectives.
6.  **Applications Survey:** Surveys several applications, including few-shot learning, reinforcement learning, and neural architecture search.
7.  **Positioning:** Meta-learning is positioned with respect to similar and related concepts like transfer learning and multi-task learning.

<details>
    <summary>主要方法</summary>
    本综述全面概述了元学习方法，重点关注当代神经网络元学习。 讨论的方法和概念包括：

    1. **高层问题形式化**：以允许在该领域内进行比较和情境化的方式定义元学习。
    2. **分类法**：本综述提出了一种使用元表示、元目标和元优化器的新分类法。
    3. **元表示**：研究了元学习的元知识选择，包括参数初始化、优化器、前馈模型、度量学习、损失/奖励函数、架构和模块。
    4. **元优化器**：讨论了外部优化技术，如梯度下降、强化学习和进化算法。
    5. **元目标和 episode 设计**：通过元目标选择来涵盖元学习目标，例如多样本与小样本 episode 以及多任务与单任务目标。
    6. **应用调查**：调查了包括小样本学习、强化学习和神经架构搜索在内的几个应用。
    7. **定位**：元学习与迁移学习和多任务学习等类似和相关的概念进行比较和定位。
</details>

**Main Contributions:**

1.  **Comprehensive Survey:** A wide-ranging survey that describes the contemporary meta-learning landscape.
2.  **New Taxonomy:** A new, more comprehensive taxonomy is proposed to better understand and classify meta-learning methods.
3.  **Application Survey:** A survey of promising applications and successes of meta-learning is provided.
4.  **Discussion of Challenges and Future Research:** Outstanding challenges and promising areas for future research are discussed.
5.  **Positioning and Differentiation:** Meta-learning is clearly positioned with respect to related fields, such as transfer learning and hyperparameter optimization.

<details>
    <summary>主要贡献</summary>
    1. **全面综述**：一篇范围广泛的综述，描述了当代元学习的概况。
    2. **新分类法**：提出了一种新的、更全面的分类法，以更好地理解和分类元学习方法。
    3. **应用调查**：提供了元学习有前景的应用和成功的调查。
    4. **对挑战和未来研究的讨论**：讨论了杰出的挑战和未来研究的有希望的领域。
    5. **定位与区分**：元学习与相关领域（如迁移学习和超参数优化）进行了清晰的定位。
</details>
