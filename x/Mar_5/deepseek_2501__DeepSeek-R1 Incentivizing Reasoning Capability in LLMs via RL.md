Okay, here is the abstract for the DeepSeek-R1 paper, following your specific instructions:

## Abstract

**Keywords:** LLMs, Reinforcement Learning, Reasoning Capability, Distillation, Multi-Stage Training, Cold Start Data

<details>
    <summary>关键词</summary>
    <ul>
        大型语言模型，强化学习，推理能力，知识蒸馏，多阶段训练，冷启动数据
    </ul>
</details>

**Abstract:** This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, a novel family of reasoning models developed by DeepSeek-AI. DeepSeek-R1-Zero leverages large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) to achieve remarkable reasoning capabilities, demonstrating the potential of pure RL for emergent reasoning. DeepSeek-R1 builds upon this foundation, incorporating multi-stage training and cold-start data before RL to address limitations in readability and language mixing, and further enhance performance. DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on reasoning benchmarks. Furthermore, the paper explores knowledge distillation from DeepSeek-R1 into smaller dense models, achieving state-of-the-art results. To support the research community, the authors open-source DeepSeek-R1-Zero, DeepSeek-R1, and several distilled dense models.

<details>
    <summary>摘要</summary>
    <ul>
       本文介绍了DeepSeek-AI开发的推理模型DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero利用大规模强化学习（RL）且不经过监督微调（SFT），从而获得卓越的推理能力，证明了纯强化学习在涌现推理方面的潜力。 DeepSeek-R1在此基础上构建，在RL之前整合了多阶段训练和冷启动数据，以解决可读性和语言混合方面的局限性，并进一步提高性能。DeepSeek-R1在推理基准测试中实现了与OpenAI-01-1217相当的性能。此外，本文还探讨了将DeepSeek-R1的知识提炼到较小的稠密模型中，实现了最先进的结果。为了支持研究社区，作者开源了DeepSeek-R1-Zero、DeepSeek-R1和几个提炼的稠密模型。
    </ul>
</details>

**Main Methods:**

1.  **Reinforcement Learning (RL) without SFT:** Training LLMs directly with RL without an initial SFT stage to discover emergent reasoning abilities.
2.  **Multi-Stage Training:** Employing a multi-stage training pipeline involving cold-start data, reasoning-oriented RL, rejection sampling and supervised fine-tuning (SFT), and a final RL stage for alignment.
3.  **Cold-Start Data:** Using a small amount of high-quality, human-annotated data to fine-tune the base model before RL, improving stability and readability.
4.  **Reasoning-Oriented RL:** Focusing RL training on tasks that require strong reasoning skills, such as coding, math, and logic.
5.  **Rejection Sampling:** Creating new SFT data by sampling multiple responses from the RL-trained model and selecting the best ones based on defined criteria.
6.  **Knowledge Distillation:** Transferring the reasoning capabilities of DeepSeek-R1 to smaller dense models through supervised fine-tuning on a dataset generated by DeepSeek-R1.

<details>
    <summary>主要方法</summary>
    <ul>
        <li>不使用SFT的强化学习（RL）：直接使用RL训练LLM，无需初始SFT阶段，以发现涌现的推理能力。</li>
        <li>多阶段训练：采用多阶段训练流程，包括冷启动数据、面向推理的RL、拒绝采样和监督微调（SFT），以及最终用于对齐的RL阶段。</li>
        <li>冷启动数据：在RL之前，使用少量高质量的人工标注数据对基础模型进行微调，从而提高稳定性和可读性。</li>
        <li>面向推理的强化学习：将RL训练集中在需要强大推理能力的任务上，例如编码、数学和逻辑。</li>
        <li>拒绝采样：通过从RL训练的模型中抽样多个响应，并根据定义的标准选择最佳响应来创建新的SFT数据。</li>
	    <li>知识蒸馏： 通过在DeepSeek-R1生成的数据集上进行监督微调，将DeepSeek-R1的推理能力转移到较小的密集模型。</li>
    </ul>
</details>

**Main Contributions:**

1.  **Pure RL for Reasoning:** Demonstrating that LLMs can develop strong reasoning capabilities through pure RL, without the need for initial SFT.
2.  **Novel Training Pipeline:** Introducing a new multi-stage training pipeline to develop LLMs with enhanced reasoning, readability, and alignment.
3.  **Comparable Performance to OpenAI-01-1217:** Achieving performance on par with a strong, proprietary model on a range of reasoning benchmarks.
4.  **Distillation to Smaller Models:** Showing that knowledge distillation is an effective method for transferring reasoning abilities from large models to smaller, more efficient models.
5.  **Open-Source Models and Data:** Releasing DeepSeek-R1-Zero, DeepSeek-R1, and several distilled models to the research community, fostering further research and development in the field.

<details>
    <summary>主要贡献</summary>
    <ul>
        <li>纯强化学习的推理： 证明了LLM可以通过纯强化学习发展出强大的推理能力，而无需初始的SFT。</li>
        <li>新型训练流程： 引入了一种新的多阶段训练流程，以开发具有增强的推理、可读性和对齐性的LLM。</li>
        <li>与OpenAI-01-1217相当的性能： 在一系列推理基准测试中，实现了与强大的专有模型相当的性能。</li>
        <li>蒸馏到更小的模型： 表明知识蒸馏是将推理能力从大型模型转移到更小、更高效的模型的有效方法。</li>
        <li>开源模型和数据： 向研究社区发布DeepSeek-R1-Zero、DeepSeek-R1 和几个蒸馏模型，从而促进该领域的进一步研究和开发。</li>
    </ul>
</details>
